# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14_HaR6sVvYZPWd3WtPrtG9urpeJ2i6Ss
"""

# =============================
# ✅ 1. Mount Google Drive
# =============================
from google.colab import drive
drive.mount('/content/drive')

# =============================
# ✅ 2. Imports
# =============================
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import random

# =============================
# ✅ 3. Use GPU if available
# =============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# =============================
# ✅ 4. DnCNN Model Definition
# =============================
class DnCNN(nn.Module):
    def __init__(self, channels=3, num_of_layers=17):
        super(DnCNN, self).__init__()
        kernel_size = 3
        padding = 1
        features = 64
        layers = []
        layers.append(nn.Conv2d(channels, features, kernel_size, padding=padding, bias=False))
        layers.append(nn.ReLU(inplace=True))

        for _ in range(num_of_layers - 2):
            layers.append(nn.Conv2d(features, features, kernel_size, padding=padding, bias=False))
            layers.append(nn.BatchNorm2d(features))
            layers.append(nn.ReLU(inplace=True))

        layers.append(nn.Conv2d(features, channels, kernel_size, padding=padding, bias=False))
        self.dncnn = nn.Sequential(*layers)

    def forward(self, x):
        out = self.dncnn(x)
        return x - out  # Residual learning

import torch
import torch.nn as nn

# =============================
# ✅ Improved DnCNN Model (22 layers + Dropout)
# =============================
class ImprovedDnCNN(nn.Module):
    def __init__(self, channels=3, num_of_layers=22):
        super(ImprovedDnCNN, self).__init__()
        kernel_size = 3
        padding = 1
        features = 96  # 🔥 increased from 64 -> 96 filters
        dropout_prob = 0.2  # 🔥 dropout 20%

        layers = []
        # First conv layer (no batchnorm)
        layers.append(nn.Conv2d(channels, features, kernel_size, padding=padding, bias=False))
        layers.append(nn.ReLU(inplace=True))

        # Middle layers
        for _ in range(num_of_layers - 2):
            layers.append(nn.Conv2d(features, features, kernel_size, padding=padding, bias=False))
            layers.append(nn.BatchNorm2d(features))
            layers.append(nn.ReLU(inplace=True))
            layers.append(nn.Dropout(dropout_prob))  # 🔥 Add dropout here

        # Last conv layer
        layers.append(nn.Conv2d(features, channels, kernel_size, padding=padding, bias=False))

        self.dncnn = nn.Sequential(*layers)

    def forward(self, x):
        out = self.dncnn(x)
        return x - out  # ✅ still using residual learning

# Instantiate
model = ImprovedDnCNN().to(device)

# =============================
# ✅ 5. Dataset Definition
# =============================
class MultiNoiseDenoisingDataset(Dataset):
    def __init__(self, clean_dir, noise_dirs, transform=None):
        self.clean_dir = clean_dir
        self.noise_dirs = noise_dirs
        self.transform = transform

        self.clean_images = sorted([f for f in os.listdir(clean_dir) if f.endswith(('.jpg', '.png'))])
        self.noisy_image_paths = []

        for noise_dir in noise_dirs:
            for img_name in self.clean_images:
                noisy_path = os.path.join(noise_dir, img_name)
                clean_path = os.path.join(clean_dir, img_name)
                if os.path.exists(noisy_path) and os.path.exists(clean_path):
                    self.noisy_image_paths.append((noisy_path, clean_path))

        random.shuffle(self.noisy_image_paths)

    def __len__(self):
        return len(self.noisy_image_paths)

    def __getitem__(self, idx):
        noisy_path, clean_path = self.noisy_image_paths[idx]

        noisy_img = Image.open(noisy_path).convert('RGB')
        clean_img = Image.open(clean_path).convert('RGB')

        if self.transform:
            noisy_img = self.transform(noisy_img)
            clean_img = self.transform(clean_img)

        return noisy_img, clean_img

# =============================
# ✅ 6. Setup Paths
# =============================
base_path = "/content/drive/MyDrive/Colob"
clean_dir = os.path.join(base_path, "clean")
noise_dirs = [
    os.path.join(base_path, "gaussian_noise")
    # os.path.join(base_path, "salt_pepper_noise")
    #os.path.join(base_path, "poisson_noise")
]

# =============================
# ✅ 7. Transform and DataLoader
# =============================
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

dataset = MultiNoiseDenoisingDataset(clean_dir, noise_dirs, transform=transform)
train_loader = DataLoader(dataset, batch_size=4, shuffle=True)

#add dat augmantation

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),   # Add random horizontal flip
    transforms.RandomVerticalFlip(),     # Add random vertical flip
    transforms.ToTensor()
])
dataset = MultiNoiseDenoisingDataset(clean_dir, noise_dirs, transform=transform)
train_loader = DataLoader(dataset, batch_size=4, shuffle=True)

# =============================
# ✅ 8. Initialize Model, Optimizer, Loss
# =============================
model = DnCNN().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

#learning rate sheduler
# 8. Initialize Model, Optimizer, Loss, Scheduler
model = ImprovedDnCNN().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)

# =============================
# ✅ 9. Training Loop
# =============================
num_epochs = 20
patience = 5
best_loss = float('inf')
counter = 0

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for noisy_imgs, clean_imgs in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        noisy_imgs = noisy_imgs.to(device)
        clean_imgs = clean_imgs.to(device)

        outputs = model(noisy_imgs)
        loss = criterion(outputs, clean_imgs)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    print(f"✅ Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

    if avg_loss < best_loss:
        best_loss = avg_loss
        counter = 0
        torch.save(model.state_dict(), os.path.join(base_path, "best_dncnn.pth"))
        print("✅ Best model saved.")
    else:
        counter += 1
        print(f"No improvement in loss for {counter} epochs.")
        if counter >= patience:
            print("⛔ Early stopping triggered!")
            break


# # =============================
# # ✅ 10. Save Final Model
# # =============================
# torch.save(model.state_dict(), os.path.join(base_path, "final_dncnn.pth"))
# print("✅ Final model saved to Drive!")


import glob

# Clean up all .pth files except the best model
all_checkpoints = glob.glob(os.path.join(base_path, "*.pth"))

for ckpt in all_checkpoints:
    if not ckpt.endswith("best_dncnn.pth"):
        os.remove(ckpt)
        print(f"🧹 Deleted temporary file: {ckpt}")

print("✅ Cleanup complete. Only best_dncnn.pth is saved.")

import matplotlib.pyplot as plt
import glob

# =============================
# ✅ 9. Updated Training Loop (with Scheduler)
# =============================
num_epochs = 100   # Increased from 20 to 100
patience = 15
best_loss = float('inf')
counter = 0

train_losses = []  # <--- to save loss values for plotting

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for noisy_imgs, clean_imgs in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        noisy_imgs = noisy_imgs.to(device)
        clean_imgs = clean_imgs.to(device)

        outputs = model(noisy_imgs)
        loss = criterion(outputs, clean_imgs)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    train_losses.append(avg_loss)  # <--- SAVE avg loss

    print(f"✅ Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

    # ✅ Step the scheduler after each epoch
    scheduler.step(avg_loss)

    if avg_loss < best_loss:
        best_loss = avg_loss
        counter = 0
        torch.save(model.state_dict(), os.path.join(base_path, "best_dncnnnew1.pth"))
        print("✅ Best model saved.")
    else:
        counter += 1
        print(f"No improvement in loss for {counter} epochs.")
        if counter >= patience:
            print("⛔ Early stopping triggered!")
            break

# =============================
# ✅ 10. Save Final Model
# =============================
torch.save(model.state_dict(), os.path.join(base_path, "final_dncnn.pth"))
print("✅ Final model saved to Drive!")

# =============================
# ✅ 11. Cleanup Temporary Files
# =============================
all_checkpoints = glob.glob(os.path.join(base_path, "*.pth"))

for ckpt in all_checkpoints:
    if not ckpt.endswith("best_dncnnnew1.pth"):
        os.remove(ckpt)
        print(f"🧹 Deleted temporary file: {ckpt}")

print("✅ Cleanup complete. Only best_dncnn.pth is saved.")

# =============================
# ✅ 12. Plot Training Loss
# =============================
plt.figure(figsize=(8,6))
plt.plot(train_losses, marker='o', label='Training Loss')
plt.title('Training Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid()
plt.legend()
plt.show()

# =============================
# ✅ 8. Initialize Model, Optimizer, Loss, Scheduler
# =============================
class CombinedLoss(nn.Module):
    def __init__(self, alpha=0.5):
        super(CombinedLoss, self).__init__()
        self.mse_loss = nn.MSELoss()
        self.l1_loss = nn.L1Loss()
        self.alpha = alpha  # Weighting factor for MSE and L1 losses

    def forward(self, output, target):
        mse = self.mse_loss(output, target)
        l1 = self.l1_loss(output, target)
        return self.alpha * mse + (1 - self.alpha) * l1

# Combine MSE and L1 loss with alpha factor
model = DnCNN().to(device)
criterion = CombinedLoss(alpha=0.5)  # Adjust alpha as necessary
optimizer = optim.Adam(model.parameters(), lr=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)

# =============================
# ✅ 9. Updated Training Loop (with Combined Loss and Scheduler)
# =============================
num_epochs = 100   # Increased from 20 to 100
patience = 15
best_loss = float('inf')
counter = 0

train_losses = []  # <--- to save loss values for plotting

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for noisy_imgs, clean_imgs in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        noisy_imgs = noisy_imgs.to(device)
        clean_imgs = clean_imgs.to(device)

        outputs = model(noisy_imgs)
        loss = criterion(outputs, clean_imgs)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    train_losses.append(avg_loss)  # <--- SAVE avg loss

    print(f"✅ Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

    # ✅ Step the scheduler after each epoch
    scheduler.step(avg_loss)

    if avg_loss < best_loss:
        best_loss = avg_loss
        counter = 0
        torch.save(model.state_dict(), os.path.join(base_path, "best_dncnnnew.pth"))
        print("✅ Best model saved.")
    else:
        counter += 1
        print(f"No improvement in loss for {counter} epochs.")
        if counter >= patience:
            print("⛔ Early stopping triggered!")
            break

# =============================
# ✅ 10. Save Final Model
# =============================
torch.save(model.state_dict(), os.path.join(base_path, "final_dncnn.pth"))
print("✅ Final model saved to Drive!")

# =============================
# ✅ 11. Cleanup Temporary Files
# =============================
all_checkpoints = glob.glob(os.path.join(base_path, "*.pth"))

for ckpt in all_checkpoints:
    if not ckpt.endswith("best_dncnnnew.pth"):
        os.remove(ckpt)
        print(f"🧹 Deleted temporary file: {ckpt}")

print("✅ Cleanup complete. Only best_dncnn.pth is saved.")

# =============================
# ✅ 12. Plot Training Loss
# =============================
plt.figure(figsize=(8,6))
plt.plot(train_losses, marker='o', label='Training Loss')
plt.title('Training Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid()
plt.legend()
plt.show()

class MultiNoiseDenoisingDataset(Dataset):
    def __init__(self, clean_dir, noise_dirs, transform=None, max_per_noise_type=None):
        self.clean_dir = clean_dir
        self.noise_dirs = noise_dirs
        self.transform = transform
        self.noisy_image_paths = []

        clean_images = sorted([f for f in os.listdir(clean_dir) if f.endswith(('.jpg', '.png'))])

        for noise_dir in noise_dirs:
            valid_pairs = []
            for img_name in clean_images:
                noisy_path = os.path.join(noise_dir, img_name)
                clean_path = os.path.join(clean_dir, img_name)
                if os.path.exists(noisy_path) and os.path.exists(clean_path):
                    valid_pairs.append((noisy_path, clean_path))

            # Limit number of images from each noise folder
            if max_per_noise_type:
                valid_pairs = valid_pairs[:max_per_noise_type]

            self.noisy_image_paths.extend(valid_pairs)

        random.shuffle(self.noisy_image_paths)

    def __len__(self):
        return len(self.noisy_image_paths)

    def __getitem__(self, idx):
        noisy_path, clean_path = self.noisy_image_paths[idx]

        noisy_img = Image.open(noisy_path).convert('RGB')
        clean_img = Image.open(clean_path).convert('RGB')

        if self.transform:
            noisy_img = self.transform(noisy_img)
            clean_img = self.transform(clean_img)

        return noisy_img, clean_img

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

# Use only first 200 images from each noise type
dataset = MultiNoiseDenoisingDataset(
    clean_dir, noise_dirs, transform=transform, max_per_noise_type=300
)
train_loader = DataLoader(dataset, batch_size=4, shuffle=True)

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import os
import numpy as np

# ============================================
# ✅ 1. Define the DnCNN Model (same as training)
# ============================================
class DnCNN(nn.Module):
    def __init__(self, channels=3, num_of_layers=17):
        super(DnCNN, self).__init__()
        kernel_size = 3
        padding = 1
        features = 64
        layers = []
        layers.append(nn.Conv2d(channels, features, kernel_size, padding=padding, bias=False))
        layers.append(nn.ReLU(inplace=True))
        for _ in range(num_of_layers - 2):
            layers.append(nn.Conv2d(features, features, kernel_size, padding=padding, bias=False))
            layers.append(nn.BatchNorm2d(features))
            layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Conv2d(features, channels, kernel_size, padding=padding, bias=False))
        self.dncnn = nn.Sequential(*layers)

    def forward(self, x):
        out = self.dncnn(x)
        return x - out  # residual learning

# ============================================
# ✅ 2. Load Trained Model
# ============================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DnCNN()
model_path = "/content/drive/MyDrive/Colob/best_dncnn.pth"  # update if needed
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()

# ============================================
# ✅ 3. Image Preprocessing
# ============================================
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

# Load a noisy image
image_path = "/content/0001.png"  # 🔁 update to your test image
noisy_img = Image.open(image_path).convert("RGB")
input_tensor = transform(noisy_img).unsqueeze(0).to(device)

# ============================================
# ✅ 4. Denoise the Image
# ============================================
with torch.no_grad():
    output_tensor = model(input_tensor)

# Convert tensors to numpy for display
output_image = output_tensor.squeeze().cpu().clamp(0, 1).numpy().transpose(1, 2, 0)
input_image = input_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)

# ============================================
# ✅ 5. Display the Results
# ============================================
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(input_image)
plt.title("Noisy Image")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(output_image)
plt.title("Denoised Image")
plt.axis("off")
plt.tight_layout()
plt.show()

# # ============================================
# # ✅ 6. Save Output Image (Optional)
# # ============================================
save_path = "/content/drive/MyDrive/Colob/denoised_result7.png"
output_pil = Image.fromarray((output_image * 255).astype(np.uint8))
output_pil.save(save_path)
print(f"✅ Denoised image saved to {save_path}")

from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

# Load a noisy image
image_path = "/content/0802.png"  # 🔁 update to your test image
noisy_img = Image.open(image_path).convert("RGB")
input_tensor = transform(noisy_img).unsqueeze(0).to(device)

# ============================================
# ✅ 4. Denoise the Image
# ============================================
with torch.no_grad():
    output_tensor = model(input_tensor)

# Convert tensors to numpy for display
output_image = output_tensor.squeeze().cpu().clamp(0, 1).numpy().transpose(1, 2, 0)
input_image = input_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)

# ============================================
# ✅ 5. Display the Results
# ============================================
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(input_image)
plt.title("Noisy Image")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(output_image)
plt.title("Denoised Image")
plt.axis("off")
plt.tight_layout()
plt.show()

# # ============================================
# # ✅ 6. Save Output Image (Optional)
# # ============================================
# save_path = "/content/drive/MyDrive/Colob/denoised_resul98.png"
# output_pil = Image.fromarray((output_image * 255).astype(np.uint8))
# output_pil.save(save_path)
# print(f"✅ Denoised image saved to {save_path}")

model = DnCNN().to(device)  # Use the same class definition you used before

checkpoint_path = "/content/drive/MyDrive/Colob/best_dncnnnew2.pth"
model.load_state_dict(torch.load(checkpoint_path))
print("✅ Pretrained weights loaded successfully!")

checkpoint_path = "/content/drive/MyDrive/Colob/best_dncnnnew1.pth"
model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu')))
print("✅ Pretrained weights loaded successfully!")

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

from PIL import Image
import os

# Verify dataset alignment
def check_alignment(clean_dir, noise_dirs, num_samples=5):
    clean_images = sorted([f for f in os.listdir(clean_dir) if f.endswith(('.jpg', '.png'))])

    for noise_dir in noise_dirs:
        print(f"\n🔎 Checking {noise_dir}...")
        for img_name in clean_images[:num_samples]:  # Check first few samples
            clean_path = os.path.join(clean_dir, img_name)
            noisy_path = os.path.join(noise_dir, img_name)

            if not os.path.exists(noisy_path):
                print(f"❌ No noisy image for {img_name} in {noise_dir}")
                continue

            # Load images
            clean_img = Image.open(clean_path)
            noisy_img = Image.open(noisy_path)

            # Check size
            if clean_img.size != noisy_img.size:
                print(f"⚠️ Size mismatch for {img_name}: Clean {clean_img.size} vs Noisy {noisy_img.size}")
            else:
                print(f"✅ {img_name} aligned correctly with size {clean_img.size}")

            clean_img.close()
            noisy_img.close()

# Run alignment check
clean_dir = "/content/drive/MyDrive/Colob/clean"
noise_dirs = [
    "/content/drive/MyDrive/Colob/gaussian_noise",
    "/content/drive/MyDrive/Colob/poisson_noise"
]

check_alignment(clean_dir, noise_dirs, num_samples=5)

from IPython.display import display

# Visual check example
sample_img = "0110.png"  # change to any filename

clean_path = os.path.join(clean_dir, sample_img)
noisy_path = os.path.join(noise_dirs[0], sample_img)  # change index for other noise types

clean_img = Image.open(clean_path)
noisy_img = Image.open(noisy_path)

print("✅ Clean Image:")
display(clean_img)

print("🎨 Noisy Image:")
display(noisy_img)

#layers 22 traning loop
import matplotlib.pyplot as plt
import glob

# =============================
# ✅ 9. Updated Training Loop (with Scheduler)
# =============================
num_epochs = 100   # Increased from 20 to 100
patience = 15
best_loss = float('inf')
counter = 0

train_losses = []  # <--- to save loss values for plotting

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for noisy_imgs, clean_imgs in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        noisy_imgs = noisy_imgs.to(device)
        clean_imgs = clean_imgs.to(device)

        outputs = model(noisy_imgs)
        loss = criterion(outputs, clean_imgs)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    train_losses.append(avg_loss)  # <--- SAVE avg loss

    print(f"✅ Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

    # ✅ Step the scheduler after each epoch
    scheduler.step(avg_loss)

    if avg_loss < best_loss:
        best_loss = avg_loss
        counter = 0
        torch.save(model.state_dict(), os.path.join(base_path, "best_dncnnnew2.pth"))
        print("✅ Best model saved.")
    else:
        counter += 1
        print(f"No improvement in loss for {counter} epochs.")
        if counter >= patience:
            print("⛔ Early stopping triggered!")
            break

# =============================
# ✅ 10. Save Final Model
# =============================
torch.save(model.state_dict(), os.path.join(base_path, "final_dncnn1.pth"))
print("✅ Final model saved to Drive!")

# =============================
# ✅ 11. Cleanup Temporary Files
# =============================
all_checkpoints = glob.glob(os.path.join(base_path, "*.pth"))

for ckpt in all_checkpoints:
    if not ckpt.endswith("best_dncnnnew2.pth"):
        os.remove(ckpt)
        print(f"🧹 Deleted temporary file: {ckpt}")

print("✅ Cleanup complete. Only best_dncnn.pth is saved.")

# =============================
# ✅ 12. Plot Training Loss
# =============================
plt.figure(figsize=(8,6))
plt.plot(train_losses, marker='o', label='Training Loss')
plt.title('Training Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid()
plt.legend()
plt.show()

from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),  # 🔥 Patch training
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.GaussianBlur(kernel_size=3),
    transforms.ToTensor()
])
dataset = MultiNoiseDenoisingDataset(clean_dir, noise_dirs, transform=transform)
train_loader = DataLoader(dataset, batch_size=4, shuffle=True)

import pytorch_ssim  # 🔥 Make sure to install: pip install pytorch-ssim


class CombinedLoss(nn.Module):
    def __init__(self, alpha=0.8):  # adjust alpha as needed
        super(CombinedLoss, self).__init__()
        self.alpha = alpha
        self.mse = nn.MSELoss()
        self.ssim = pytorch_ssim.SSIM()

    def forward(self, output, target):
        return self.alpha * self.mse(output, target) + (1 - self.alpha) * (1 - self.ssim(output, target))

from torch.optim.lr_scheduler import OneCycleLR

optimizer = optim.Adam(model.parameters(), lr=1e-4)
scheduler = OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=100)

criterion = CombinedLoss(alpha=0.8)  # using new combined loss
num_epochs = 100
patience = 15
best_loss = float('inf')
counter = 0
train_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for noisy_imgs, clean_imgs in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        noisy_imgs, clean_imgs = noisy_imgs.to(device), clean_imgs.to(device)

        optimizer.zero_grad()
        outputs = model(noisy_imgs)
        loss = criterion(outputs, clean_imgs)
        loss.backward()
        optimizer.step()
        scheduler.step()  # ✅ Use OneCycle step per batch

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    train_losses.append(avg_loss)
    print(f"✅ Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

    if avg_loss < best_loss:
        best_loss = avg_loss
        counter = 0
        torch.save(model.state_dict(), os.path.join(base_path, "best_dncnnnew3.pth"))
        print("✅ Best model saved.")
    else:
        counter += 1
        print(f"No improvement in loss for {counter} epochs.")
        if counter >= patience:
            print("⛔ Early stopping triggered!")
            break

pip install pytorch-ssim

pip show pytorch-ssim

!cat /usr/local/lib/python3.11/dist-packages/pytorch_ssim/__init__.py